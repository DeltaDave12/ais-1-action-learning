{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21655b86-cba1-4db2-8c28-c1bd2a2850cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Acer\\Desktop\\AL-Emotion\\models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3ec55e-74c7-4bc4-8c1d-130c5e18191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\miniconda3\\envs\\AI_git\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d155d89-3b31-4bf6-85fd-31a80defd06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU Configuration\n",
      "==================================================\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Number of GPUs: 1\n",
      "==================================================\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\miniconda3\\envs\\AI_git\\Lib\\site-packages\\torch\\__init__.py:1240: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing enhanced features...\n",
      "Enhanced feature dimensions: 3000\n",
      "Predicted Emotion: frustrated\n",
      "Predicted Strategy: RAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\miniconda3\\envs\\AI_git\\Lib\\site-packages\\transformers\\quantizers\\auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mistral 7B Answer:\n",
      "\n",
      "\n",
      "Neural networks are a subset of artificial intelligence (AI) models that are designed to mimic the way the human brain processes information. They consist of interconnected nodes or neurons that process information using a series of weighted connections.\n",
      "\n",
      "The basic building block of a neural network is a single neuron, which takes in a set of inputs, applies a non-linear activation function, and produces an output. Multiple neurons are then connected together to form a layer, and multiple layers are stacked on top of each other to form a neural network.\n",
      "\n",
      "During training, the network adjusts the weights of the connections between neurons based on the error of the network's predictions. This process is repeated many times until the network's predictions are as accurate as possible.\n",
      "\n",
      "Neural networks can be used for a variety of tasks, including image recognition, speech recognition, language translation, and even playing games. They are particularly effective at tasks that involve understanding complex patterns or relationships in data.\n",
      "\n",
      "If you are having trouble understanding neural networks, I would recommend starting with some basic concepts, such as:\n",
      "\n",
      "* What is a neuron in a neural network?\n",
      "* What is a layer in a neural network?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from improved_emotion_recognition import ImprovedEmotionRecognitionModel, ImprovedTransformerEmotionModel\n",
    "from progressive_emotion_training import device\n",
    "import pandas as pd\n",
    "\n",
    "# Load and run emotion model\n",
    "emotion_ckpt = \"../models/progressive_model_stage2.pth\"\n",
    "checkpoint = torch.load(emotion_ckpt, weights_only=False)\n",
    "emotion_model = ImprovedEmotionRecognitionModel()\n",
    "tfidf_features = len(checkpoint['vectorizers']['tfidf'].get_feature_names_out())\n",
    "count_features = len(checkpoint['vectorizers']['count'].get_feature_names_out())\n",
    "total_features = tfidf_features + count_features\n",
    "emotion_model.model = ImprovedTransformerEmotionModel(\n",
    "    text_feature_size=total_features,\n",
    "    num_classes=len(checkpoint['label_encoder'].classes_),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "emotion_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "emotion_model.tfidf_vectorizer = checkpoint['vectorizers']['tfidf']\n",
    "emotion_model.count_vectorizer = checkpoint['vectorizers']['count']\n",
    "emotion_model.label_encoder = checkpoint['label_encoder']\n",
    "emotion_model.model.eval()\n",
    "\n",
    "query = \"I do not understand neural networks\"\n",
    "demo_df = pd.DataFrame({'current_query': [query]})\n",
    "features = emotion_model.enhanced_feature_preparation(demo_df, fit_vectorizer=False, augment=False)\n",
    "input_tensor = torch.FloatTensor(features).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = emotion_model.model(input_tensor)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "emotion = emotion_model.label_encoder.classes_[predicted_class]\n",
    "print(f\"Predicted Emotion: {emotion}\")\n",
    "\n",
    "# Free memory\n",
    "del emotion_model, checkpoint, features, input_tensor, outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- Step 2: MLP Model ---\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "mlp_model = joblib.load(\"../models/mlp_classifier.pkl\")\n",
    "mlp_label_encoder = joblib.load(\"../models/label_encoder.pkl\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "query_embedding = sentence_model.encode([query])\n",
    "history_count_norm = 0.0\n",
    "mlp_input = np.hstack([query_embedding, [[history_count_norm]]])\n",
    "strategy_idx = mlp_model.predict(mlp_input)[0]\n",
    "strategy = mlp_label_encoder.inverse_transform([strategy_idx])[0]\n",
    "print(f\"Predicted Strategy: {strategy}\")\n",
    "\n",
    "# Free memory\n",
    "del mlp_model, mlp_label_encoder, sentence_model, query_embedding, mlp_input, strategy_idx\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- Step 3: Mistral 7B Model ---\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../models/mistral_7b_quantized\",\n",
    "    device_map=\"cuda\",  # or \"cuda\" if you have enough VRAM\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"../models/mistral_7b_quantized\")\n",
    "mistral_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=mistral_model,\n",
    "    tokenizer=mistral_tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "mistral_prompt = (\n",
    "    f\"User prompt: {query}\\n\"\n",
    "    f\"Emotion: {emotion}\\n\"\n",
    "    f\"Content decision: Use {strategy}\\n\"\n",
    "    \"Please provide a comprehensive answer.\"\n",
    ")\n",
    "answer = mistral_pipe(mistral_prompt)[0]['generated_text']\n",
    "print(f\"\\nMistral 7B Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162b320-8f94-433a-b8a1-cf4dcd26cb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
