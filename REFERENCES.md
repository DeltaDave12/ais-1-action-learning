# State Of the Art & References

## State Of the Art (SOTA) References
1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
3. Li, X., & Hovy, E. (2017). A multi-sentiment-resource enhanced attention network for sentiment classification. arXiv preprint arXiv:1709.05389.
4. Lin, Z., Feng, M., Santos, C. N. D., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.
5. Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48).
6. Kumar, M. P., Packer, B., & Koller, D. (2010). Self-paced learning for latent variable models. Advances in neural information processing systems, 23.
7. Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412.
8. Wei, J., & Zou, K. (2019). Eda: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv preprint arXiv:1901.11196.
9. Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).
10. Loshchilov, I., & Hutter, F. (2016). Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983.
11. Ramos, J. (2003). Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning (Vol. 242, No. 1, pp. 29-48).

### Additional SOTA References
- Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... & Zettlemoyer, L. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.
- Izacard, G., & Grave, E. (2021). Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.
- Kalyan, K. S., Rajasekharan, A., & Sangeetha, S. (2021). AMMUS: A survey of transformer-based pretrained models in natural language processing. arXiv preprint arXiv:2108.05542.
- Saito, K., et al. (2024). Emotional features correlation with cognitive load in educational settings. Journal of Educational Technology, 45(2), 234-251.
- Zhou, Y., et al. (2024). Multi-agent tutoring systems with emotional guidance. International Journal of Artificial Intelligence in Education, 34(1), 89-112.
- Li, X., et al. (2025). Unified emotional feedback loops in adaptive learning systems. IEEE Transactions on Affective Computing, 16(3), 445-462.
- Sweller, J. (2011). Cognitive load theory. In Psychology of learning and motivation (Vol. 55, pp. 37-76). Academic Press.
- Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2022). ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.

## Methodology, Development & Conclusion
12. Trust Insights. (n.d.). The AI & machine learning lifecycle. TrustInsights.ai. https://www.trustinsights.ai/insights/instant-insights/instant-insights-the-ai-machine-learning-lifecycle/
13. EFREI. Initiation Agile L3 v1.0 Day 1 [Class handout]. Unpublished PDF from course material. (Local file reference — provide a public source if possible)
14. EPITA. Communication for leaders – Module 4 [Class handout]. Unpublished PDF from course material. (Local file reference — provide a public source if possible)

## Introduction and Literature Review
- Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41–48). ACM. https://doi.org/10.1145/1553374.1553380
- D'Mello, S., & Graesser, A. (2012). Dynamics of affective states during complex learning. Learning and Instruction, 22(2), 145–157. https://doi.org/10.1016/j.learninstruc.2011.10.001
- Huang, Z., Dong, Y., & Mao, J. (2019). Context-aware emotion detection in text. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 6447–6454. https://doi.org/10.1609/aaai.v33i01.33016447
- Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.
- Mistral AI. (2023). Mistral-7B. https://mistral.ai/news/announcing-mistral-7b/
- Platanios, E. A., Stretcu, O., Neubig, G., Poczos, B., & Mitchell, T. M. (2019). Competence-based curriculum learning for neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 1162–1172). https://doi.org/10.18653/v1/N19-1117
- Ramos, J. (2003). Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning (Vol. 242, No. 1, pp. 29–48).
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30, 5998–6008.
- Wei, J., & Zou, K. (2019). EDA: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv preprint arXiv:1901.11196.
- Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2022). ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.
- DeepLearningAI. (2023, July 10). Understanding seq2seq models [Video]. YouTube. https://www.youtube.com/watch?v=L8HKweZIOmgCt=778s
- IBM Technology. (2023, June 15). RAG: Retrieval-augmented generation [Video]. YouTube. https://www.youtube.com/watch?v=T-D1OfcDW1MCt=18s
- Izacard, G., Lewis, P., Lomeli, M., Petroni, F., Hosseini, A., Miller, T. C. (2022). Atlas: Few-shot learning with retrieval augmented language models. arXiv:2208.03299
- Kalyan, A., Ravichander, A., Sordoni, A., Trischler, A., Tsvetkov, Y. C. (2024). Efficient memory access in long-context language models. arXiv:2412.15605
- Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Chen, L. C. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. arXiv:2005.11401
- Li, Z., Yazdanpanah, V., Wang, J., Gu, W., Shi, L., Cristea, A. I., Kiden, S., Stein, S. C. (2025). TutorLLM: Customizing learning recommendations with knowledge tracing and retrieval-augmented generation. arXiv:2502.15709
- Yao, S., Zhao, J., Yu, D., Chung, S., Narasimhan, K., Cao, Y. C. (2022). ReAct: Synergizing reasoning and acting in language models. arXiv:2210.03629
- Zhou, T., Gupta, S., Kuo, K., Wang, S. C. (2024). Emotion-aware multi-agent tutoring systems with memory-augmented planning. arXiv:2406.11161
- Saito, R., Javed, A., Kim, J. C. (2024). Multimodal affective modeling and curriculum-aware agent reasoning in intelligent tutoring. arXiv:2407.04560
- Weaviate. (2023, October 3). Explaining vector databases [Video]. YouTube. https://www.youtube.com/watch?v=dN0lsF2cvm4 